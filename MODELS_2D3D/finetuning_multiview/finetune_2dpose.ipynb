{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4fbcae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865e705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lea/anaconda3/envs/multiview/lib/python3.10/site-packages/mmengine/utils/package_utils.py:48: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.cuda.amp.autocast.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.meshgrid.*\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/lea/trampo/MODELS_2D3D/mmpose')\n",
    "sys.path.append('/home/lea/trampo/metrics')\n",
    "\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "\n",
    "from utils import predict_multiview_with_grad, find_best_triangulation, project_points, find_triangulation, show_keypoints_on_im, resize_and_pad_keep_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c117a",
   "metadata": {},
   "source": [
    "### Set models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8469a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lea/anaconda3/envs/multiview/lib/python3.10/importlib/__init__.py:169: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  _bootstrap._exec(spec, module)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.pth\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cuda' # FOR TESTING\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "N = 8 # number of cameras\n",
    "K = 17 # number of keypoints\n",
    "\n",
    "# Init detector\n",
    "det_config = '/home/lea/trampo/MODELS_2D3D/mmpose/demo/mmdetection_cfg/rtmdet_m_640-8xb32_coco-person.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth'\n",
    "#det_config = \"/home/lea/trampo/MODELS_2D3D/rtmpose/RTMPose/rtmdet_nano_320-8xb32_coco-person.py\" \n",
    "#det_checkpoint = \"/home/lea/trampo/MODELS_2D3D/rtmpose/RTMPose/rtmdet_nano_8xb32-100e_coco-obj365-person-05d8511e.pth\"\n",
    "det_model= init_detector(det_config, det_checkpoint, device=device)\n",
    "det_model.cfg = adapt_mmdet_pipeline(det_model.cfg)\n",
    "\n",
    "# Init pose model\n",
    "pose_config = '/home/lea/trampo/MODELS_2D3D/mmpose/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-256x192.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.pth'\n",
    "#pose_config = \"/home/lea/trampo/MODELS_2D3D/rtmpose/RTMPose/rtmpose-l_8xb256-420e_coco-256x192.py\"  \n",
    "#pose_checkpoint = \"/home/lea/trampo/MODELS_2D3D/rtmpose/RTMPose/rtmpose-l_simcc-aic-coco_pt-aic-coco_420e-256x192-f016ffe0_20230126.pth\"\n",
    "pose_model = init_pose_estimator(pose_config, pose_checkpoint, device=device)\n",
    "pose_model.train()\n",
    "\n",
    "# Unfreeze all parameters\n",
    "for param in pose_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "optimizer = torch.optim.Adam(pose_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af2050",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048e4c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for seq in sequences:\\n    if not os.path.isdir(os.path.join(data_dir, seq)):\\n        os.makedirs(os.path.join(data_dir, seq))\\n    for cam in cameras:\\n        if not os.path.isdir(os.path.join(data_dir, seq, cam)):\\n            dest_dir = os.path.join(data_dir, seq, cam)\\n            source_dir = os.path.join(root_dir, seq+'-'+cam)\\n            shutil.copytree(source_dir, dest_dir) \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/20250429'\n",
    "data_dir = '/home/lea/trampo/MODELS_2D3D/finetuning_multiview/dataset'\n",
    "\n",
    "sequences = set([str(f).split('-')[0] for f in os.listdir(root_dir)])\n",
    "\n",
    "sequences = sorted([seq for seq in sequences if seq[0] in ['1', '2']])\n",
    "\n",
    "cameras = ['Camera1_M11139', 'Camera2_M11140', 'Camera3_M11141', 'Camera4_M11458',\n",
    "           'Camera5_M11459', 'Camera6_M11461', 'Camera7_M11462', 'Camera8_M11463']\n",
    "\n",
    "K = np.load('calib/K.npz')['arr_0']\n",
    "Ks = torch.tensor(K, dtype=torch.float32, device=device)\n",
    "\n",
    "D = np.load('calib/D.npz')['arr_0']\n",
    "\n",
    "\"\"\" for seq in sequences:\n",
    "    if not os.path.isdir(os.path.join(data_dir, seq)):\n",
    "        os.makedirs(os.path.join(data_dir, seq))\n",
    "    for cam in cameras:\n",
    "        if not os.path.isdir(os.path.join(data_dir, seq, cam)):\n",
    "            dest_dir = os.path.join(data_dir, seq, cam)\n",
    "            source_dir = os.path.join(root_dir, seq+'-'+cam)\n",
    "            shutil.copytree(source_dir, dest_dir) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d43993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for seq in os.listdir(dataset_path):\\n    print(seq)\\n    person_id = int(str(seq).split(\\'_\\')[0])\\n\\n    session = str(seq).split(\\'-\\')[0].split(\\'_\\')[2]\\n    calib_path = os.path.join(\\'calib\\', f\\'WorldTCam_{session}.npz\\')\\n    world_T_cam = np.load(calib_path)[\\'arr_0\\']\\n    projMat = np.stack([np.linalg.inv(mat) for mat in world_T_cam])\\n    Ts = torch.tensor(projMat, dtype=torch.float32, device=device).unsqueeze(0)\\n    \\n    for frame in tqdm(sorted(list(os.listdir(os.path.join(dataset_path, seq, \\'Camera1_M11139\\'))))):\\n        frame_nb = int(str(frame).split(\\'_\\')[1].split(\\'.\\')[0])\\n\\n        images = []\\n        for cam in os.listdir(os.path.join(dataset_path, seq)):\\n            cam_idx = int(str(cam)[6]) - 1\\n            img_path = os.path.join(dataset_path, seq, cam, frame)\\n            img = cv2.imread(img_path)\\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\\n            images.append(img)\\n        images = torch.stack(images).unsqueeze(0)\\n\\n        with torch.no_grad():\\n            keypoints = predict_multiview_with_grad(det_model, pose_model, images, bbox_thr=0.3, pose_batch_size=8, training=False)\\n            error, preds_2d, points_3d = find_triangulation(keypoints, Ks, Ts, error_thresh)\\n        \\n        if torch.isnan(points_3d).all():\\n            continue\\n\\n        # Reproject and add cameras to cams_on if error is below error_thresh\\n        Rt = Ts[:, :, :3, :]\\n        P_all = Ks @ Rt\\n        reproj, valid_mask = project_points(points_3d, P_all)\\n\\n        dist = torch.norm(preds_2d - reproj, dim=-1).view(8, 17)  # Euclidean distance\\n        keep_mask = (dist < person_dist_thresh).all(dim=1)\\n        \\n        if keep_mask.sum() == 0:\\n            continue\\n        cams_on_all = keep_mask\\n        \\n        for cam_i, cam_on in enumerate(cams_on_all.squeeze()):\\n            if cam_on:\\n                #print((seq, cam_i, frame_nb, person_id))\\n                detections.append((seq, cam_i, frame_nb, person_id))\\n\\ndf = pd.DataFrame(detections)\\ndf.columns = [\"seq\", \"cam\", \"frame\", \"person\"]\\ndf.to_pickle(\"detections.pkl\")\\n\\nprint(df) '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inventaire athlète/frame/caméra/seq\n",
    "error_thresh = 500\n",
    "person_dist_thresh = 500  # distance threshold to discard mismatched persons\n",
    "\n",
    "dataset_path = '/home/lea/trampo/MODELS_2D3D/finetuning_multiview/dataset'\n",
    "detections = []\n",
    "\n",
    "\"\"\" for seq in os.listdir(dataset_path):\n",
    "    print(seq)\n",
    "    person_id = int(str(seq).split('_')[0])\n",
    "\n",
    "    session = str(seq).split('-')[0].split('_')[2]\n",
    "    calib_path = os.path.join('calib', f'WorldTCam_{session}.npz')\n",
    "    world_T_cam = np.load(calib_path)['arr_0']\n",
    "    projMat = np.stack([np.linalg.inv(mat) for mat in world_T_cam])\n",
    "    Ts = torch.tensor(projMat, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for frame in tqdm(sorted(list(os.listdir(os.path.join(dataset_path, seq, 'Camera1_M11139'))))):\n",
    "        frame_nb = int(str(frame).split('_')[1].split('.')[0])\n",
    "\n",
    "        images = []\n",
    "        for cam in os.listdir(os.path.join(dataset_path, seq)):\n",
    "            cam_idx = int(str(cam)[6]) - 1\n",
    "            img_path = os.path.join(dataset_path, seq, cam, frame)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "            images.append(img)\n",
    "        images = torch.stack(images).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            keypoints = predict_multiview_with_grad(det_model, pose_model, images, bbox_thr=0.3, pose_batch_size=8, training=False)\n",
    "            error, preds_2d, points_3d = find_triangulation(keypoints, Ks, Ts, error_thresh)\n",
    "        \n",
    "        if torch.isnan(points_3d).all():\n",
    "            continue\n",
    "\n",
    "        # Reproject and add cameras to cams_on if error is below error_thresh\n",
    "        Rt = Ts[:, :, :3, :]\n",
    "        P_all = Ks @ Rt\n",
    "        reproj, valid_mask = project_points(points_3d, P_all)\n",
    "\n",
    "        dist = torch.norm(preds_2d - reproj, dim=-1).view(8, 17)  # Euclidean distance\n",
    "        keep_mask = (dist < person_dist_thresh).all(dim=1)\n",
    "        \n",
    "        if keep_mask.sum() == 0:\n",
    "            continue\n",
    "        cams_on_all = keep_mask\n",
    "        \n",
    "        for cam_i, cam_on in enumerate(cams_on_all.squeeze()):\n",
    "            if cam_on:\n",
    "                #print((seq, cam_i, frame_nb, person_id))\n",
    "                detections.append((seq, cam_i, frame_nb, person_id))\n",
    "\n",
    "df = pd.DataFrame(detections)\n",
    "df.columns = [\"seq\", \"cam\", \"frame\", \"person\"]\n",
    "df.to_pickle(\"detections.pkl\")\n",
    "\n",
    "print(df) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f7bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cropped_dataset(detector, root_dir, bbox_thr, save_root=\"datasets/cropped_multiview\", model_input_size=(256, 192)):\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "    detector.eval()\n",
    "\n",
    "    for seq in tqdm(sorted(os.listdir(root_dir)), desc=\"Generating cropped dataset\"):\n",
    "        seq_path = os.path.join(save_root, seq)\n",
    "        os.makedirs(seq_path, exist_ok=True)\n",
    "\n",
    "        # Skip seq if already processed\n",
    "        if os.path.isfile(os.path.join(seq_path, f\"{seq}_meta.pt\")):\n",
    "            continue\n",
    "\n",
    "        meta_seq = []\n",
    "\n",
    "        for cam_dir in sorted(os.listdir(os.path.join(root_dir, seq))):\n",
    "            cam_path = os.path.join(seq_path, cam_dir)\n",
    "            os.makedirs(cam_path, exist_ok=True)\n",
    "\n",
    "            for frame in sorted(os.listdir(os.path.join(root_dir, seq, cam_dir))):\n",
    "                img = cv2.imread(os.path.join(root_dir, seq, cam_dir, frame))\n",
    "\n",
    "                det_result = inference_detector(detector, img)\n",
    "                pred = det_result.pred_instances.cpu().numpy()\n",
    "                bboxes = pred.bboxes[np.logical_and(pred.labels == 0, pred.scores > bbox_thr)]\n",
    "\n",
    "                for pid, bbox in enumerate(bboxes):\n",
    "                    x1, y1, x2, y2 = map(int, bbox)\n",
    "                    if x2 <= x1 or y2 <= y1:\n",
    "                        continue\n",
    "\n",
    "                    crop = img[y1:y2, x1:x2]\n",
    "                    crop_resized, scale, pads = resize_and_pad_keep_aspect(crop, model_input_size)\n",
    "\n",
    "                    crop_path = os.path.join(cam_path, frame.split('.')[0] + f\"_person{pid:02d}.jpg\")\n",
    "                    cv2.imwrite(crop_path, crop_resized)\n",
    "\n",
    "                    meta_seq.append({\n",
    "                        \"camera\": cam_dir,\n",
    "                        \"frame\": frame.split('.')[0],\n",
    "                        \"person\": pid,\n",
    "                        \"origin\": [x1, y1],\n",
    "                        \"scale\": scale,\n",
    "                        \"pads\": pads,\n",
    "                        \"crop_path\": os.path.relpath(crop_path, save_root)\n",
    "                    })\n",
    "        \n",
    "        torch.save(meta_seq, os.path.join(seq_path, f\"{seq}_meta.pt\"))\n",
    "\n",
    "# Create cropped dataset\n",
    "save_path = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/dataset_finetune2d/cropped'\n",
    "#create_cropped_dataset(det_model, root_dir, bbox_thr=0.3, save_root=save_path, model_input_size=(256, 192))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9837dea",
   "metadata": {},
   "source": [
    "### Detections stats on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eaea377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Load your detections\\ndf = pd.read_pickle(\"detections.pkl\")\\n\\nprint(\"Total detections:\", len(df))\\nprint(\"Unique sequences:\", df[\"seq\"].nunique())\\n\\n# --- Base stats per sequence ---\\nstats = df.groupby(\"seq\").agg(\\n    n_frames=(\"frame\", \"nunique\"),\\n    n_cameras=(\"cam\", \"nunique\"),\\n    n_persons=(\"person\", \"nunique\"),\\n    total_detections=(\"frame\", \"count\"),\\n).reset_index()\\n\\n# --- Camera coverage per sequence ---\\n# Count detections per seq, cam, frame\\ncoverage = df.groupby([\"seq\", \"cam\", \"frame\"]).size().reset_index(name=\"count\")\\n\\n# For each (seq, cam), count frames with detections\\ncoverage = coverage.groupby([\"seq\", \"cam\"]).agg(frames_with_detections=(\"frame\", \"nunique\")).reset_index()\\n\\n# Merge with total frames per sequence to get % coverage\\ncoverage = coverage.merge(stats[[\"seq\", \"n_frames\"]], on=\"seq\", how=\"left\")\\ncoverage[\"coverage_ratio\"] = 100 * coverage[\"frames_with_detections\"] / coverage[\"n_frames\"]\\n\\n# Pivot for readability (each camera in its own column)\\ncoverage_pivot = coverage.pivot(index=\"seq\", columns=\"cam\", values=\"coverage_ratio\").fillna(0)\\ncoverage_pivot.columns = [f\"cam{int(c)+1}_coverage(%)\" for c in coverage_pivot.columns]\\n\\n# Merge coverage with global stats\\nstats_full = stats.merge(coverage_pivot, on=\"seq\", how=\"left\")\\n\\n# --- Results ---\\nprint(\"\\n=== Detection Stats per Sequence ===\")\\nprint(stats_full.round(2))\\n\\n# Save if useful\\nstats_full.to_csv(\"detections_stats_with_coverage.csv\", index=False)\\n '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Load your detections\n",
    "df = pd.read_pickle(\"detections.pkl\")\n",
    "\n",
    "print(\"Total detections:\", len(df))\n",
    "print(\"Unique sequences:\", df[\"seq\"].nunique())\n",
    "\n",
    "# --- Base stats per sequence ---\n",
    "stats = df.groupby(\"seq\").agg(\n",
    "    n_frames=(\"frame\", \"nunique\"),\n",
    "    n_cameras=(\"cam\", \"nunique\"),\n",
    "    n_persons=(\"person\", \"nunique\"),\n",
    "    total_detections=(\"frame\", \"count\"),\n",
    ").reset_index()\n",
    "\n",
    "# --- Camera coverage per sequence ---\n",
    "# Count detections per seq, cam, frame\n",
    "coverage = df.groupby([\"seq\", \"cam\", \"frame\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# For each (seq, cam), count frames with detections\n",
    "coverage = coverage.groupby([\"seq\", \"cam\"]).agg(frames_with_detections=(\"frame\", \"nunique\")).reset_index()\n",
    "\n",
    "# Merge with total frames per sequence to get % coverage\n",
    "coverage = coverage.merge(stats[[\"seq\", \"n_frames\"]], on=\"seq\", how=\"left\")\n",
    "coverage[\"coverage_ratio\"] = 100 * coverage[\"frames_with_detections\"] / coverage[\"n_frames\"]\n",
    "\n",
    "# Pivot for readability (each camera in its own column)\n",
    "coverage_pivot = coverage.pivot(index=\"seq\", columns=\"cam\", values=\"coverage_ratio\").fillna(0)\n",
    "coverage_pivot.columns = [f\"cam{int(c)+1}_coverage(%)\" for c in coverage_pivot.columns]\n",
    "\n",
    "# Merge coverage with global stats\n",
    "stats_full = stats.merge(coverage_pivot, on=\"seq\", how=\"left\")\n",
    "\n",
    "# --- Results ---\n",
    "print(\"\\n=== Detection Stats per Sequence ===\")\n",
    "print(stats_full.round(2))\n",
    "\n",
    "# Save if useful\n",
    "stats_full.to_csv(\"detections_stats_with_coverage.csv\", index=False)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98892586",
   "metadata": {},
   "source": [
    "### Create dataset + dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c53090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len = 1094\n",
      "torch.Size([8, 3, 3, 256, 192])\n",
      "[[{'camera': 'Camera1_M11139', 'frame': 'frame_00005.png', 'person': 0, 'origin': [114, 0], 'scale': 0.5680473372781065, 'pads': (0, 85), 'crop_path': '1_partie_0429_000/Camera1_M11139/frame_00005_person00.jpg'}, None, None], [{'camera': 'Camera2_M11140', 'frame': 'frame_00005.png', 'person': 0, 'origin': [501, 622], 'scale': 1.136094674556213, 'pads': (0, 87), 'crop_path': '1_partie_0429_000/Camera2_M11140/frame_00005_person00.jpg'}, {'camera': 'Camera2_M11140', 'frame': 'frame_00005.png', 'person': 2, 'origin': [523, 324], 'scale': 1.4545454545454546, 'pads': (0, 88), 'crop_path': '1_partie_0429_000/Camera2_M11140/frame_00005_person02.jpg'}, {'camera': 'Camera2_M11140', 'frame': 'frame_00005.png', 'person': 1, 'origin': [484, 367], 'scale': 2.3132530120481927, 'pads': (0, 10), 'crop_path': '1_partie_0429_000/Camera2_M11140/frame_00005_person01.jpg'}], [{'camera': 'Camera3_M11141', 'frame': 'frame_00005.png', 'person': 0, 'origin': [0, 371], 'scale': 0.37813884785819796, 'pads': (80, 0), 'crop_path': '1_partie_0429_000/Camera3_M11141/frame_00005_person00.jpg'}, {'camera': 'Camera3_M11141', 'frame': 'frame_00005.png', 'person': 1, 'origin': [893, 688], 'scale': 0.47761194029850745, 'pads': (0, 106), 'crop_path': '1_partie_0429_000/Camera3_M11141/frame_00005_person01.jpg'}, None], [{'camera': 'Camera4_M11458', 'frame': 'frame_00005.png', 'person': 0, 'origin': [772, 6], 'scale': 1.0726256983240223, 'pads': (0, 89), 'crop_path': '1_partie_0429_000/Camera4_M11458/frame_00005_person00.jpg'}, {'camera': 'Camera4_M11458', 'frame': 'frame_00005.png', 'person': 2, 'origin': [828, 336], 'scale': 2.4615384615384617, 'pads': (0, 3), 'crop_path': '1_partie_0429_000/Camera4_M11458/frame_00005_person02.jpg'}, {'camera': 'Camera4_M11458', 'frame': 'frame_00005.png', 'person': 1, 'origin': [189, 620], 'scale': 0.6643598615916955, 'pads': (0, 111), 'crop_path': '1_partie_0429_000/Camera4_M11458/frame_00005_person01.jpg'}], [{'camera': 'Camera5_M11459', 'frame': 'frame_00005.png', 'person': 0, 'origin': [395, 292], 'scale': 0.5714285714285714, 'pads': (0, 101), 'crop_path': '1_partie_0429_000/Camera5_M11459/frame_00005_person00.jpg'}, None, None], [{'camera': 'Camera6_M11461', 'frame': 'frame_00005.png', 'person': 0, 'origin': [1335, 286], 'scale': 0.6632124352331606, 'pads': (58, 0), 'crop_path': '1_partie_0429_000/Camera6_M11461/frame_00005_person00.jpg'}, {'camera': 'Camera6_M11461', 'frame': 'frame_00005.png', 'person': 1, 'origin': [1336, 286], 'scale': 1.9104477611940298, 'pads': (3, 0), 'crop_path': '1_partie_0429_000/Camera6_M11461/frame_00005_person01.jpg'}, None], [{'camera': 'Camera7_M11462', 'frame': 'frame_00005.png', 'person': 0, 'origin': [1211, 491], 'scale': 0.41113490364025695, 'pads': (0, 98), 'crop_path': '1_partie_0429_000/Camera7_M11462/frame_00005_person00.jpg'}, None, None], [{'camera': 'Camera8_M11463', 'frame': 'frame_00005.png', 'person': 0, 'origin': [809, 397], 'scale': 0.6530612244897959, 'pads': (0, 97), 'crop_path': '1_partie_0429_000/Camera8_M11463/frame_00005_person00.jpg'}, {'camera': 'Camera8_M11463', 'frame': 'frame_00005.png', 'person': 1, 'origin': [0, 1015], 'scale': 2.430379746835443, 'pads': (0, 50), 'crop_path': '1_partie_0429_000/Camera8_M11463/frame_00005_person01.jpg'}, None]]\n"
     ]
    }
   ],
   "source": [
    "class CroppedMultiViewDataset(Dataset):\n",
    "    def __init__(self, cropped_dir, raw_dir, K, downsample=1):\n",
    "        self.cropped_dir = cropped_dir\n",
    "        self.raw_dir = raw_dir\n",
    "        self.downsample = max(1, downsample)  # avoid division by zero\n",
    "\n",
    "        sequences = sorted([d for d in os.listdir(cropped_dir) if os.path.isdir(os.path.join(cropped_dir, d))])\n",
    "        self.sequences = sorted(sequences)\n",
    "        \n",
    "        self.sequence_data = []  # (seq_name, frame_names, calibration)\n",
    "        self.index_map = []      # global index -> (seq_idx, frame_idx)\n",
    "\n",
    "        for seq_idx, seq_name in enumerate(self.sequences):\n",
    "            seq_path = os.path.join(cropped_dir, seq_name)\n",
    "\n",
    "            # Calibration\n",
    "            session = seq_name.split('-')[0].split('_')[2]\n",
    "            calib_path = os.path.join('calib', f'WorldTCam_{session}.npz')\n",
    "\n",
    "            world_T_cam = np.load(calib_path)['arr_0']\n",
    "            projMat = np.stack([np.linalg.inv(mat) for mat in world_T_cam])\n",
    "            Ts = torch.tensor(projMat, dtype=torch.float32)\n",
    "            Ks = torch.tensor(K, dtype=torch.float32)\n",
    "\n",
    "            # Cameras\n",
    "            cam_dirs = sorted([d for d in os.listdir(seq_path) if d.startswith(\"Cam\")])\n",
    "            cam_dirs = [os.path.join(seq_path, d) for d in cam_dirs]\n",
    "\n",
    "            # Collect all unique frame base names across all cameras\n",
    "            frame_names = sorted({\n",
    "                (f.split('_p')[0], f.split('_')[2])  # (frame, person_id)\n",
    "                for cam_dir in cam_dirs\n",
    "                for f in os.listdir(cam_dir)\n",
    "                if f.endswith('.jpg')\n",
    "            })\n",
    "            frame_names = frame_names[::self.downsample]  # downsample by taking every Nth frame\n",
    "\n",
    "            frame_to_persons = defaultdict(list)\n",
    "            for cam_dir in cam_dirs:\n",
    "                for f in os.listdir(cam_dir):\n",
    "                    if f.endswith('.jpg'):\n",
    "                        frame_name = f.split('_p')[0]\n",
    "                        frame_nb = int(f.split('_')[1])  # get 0001 from 'frame_0001_p01.jpg'\n",
    "                        if frame_nb % downsample == 0:\n",
    "                            person_id = f.split('_')[2]  # get 1 from 'p01.jpg'\n",
    "                            if person_id not in frame_to_persons[frame_name]:\n",
    "                                frame_to_persons[frame_name].append(person_id)\n",
    "            \n",
    "            frame_names = sorted(frame_to_persons.keys())\n",
    "            frame_to_persons = {k: frame_to_persons[k] for k in frame_names}\n",
    "\n",
    "            self.sequence_data.append({\n",
    "                \"name\": seq_name,\n",
    "                \"cam_dirs\": cam_dirs,\n",
    "                \"frame_names\": frame_to_persons,\n",
    "                \"Ks\": Ks,\n",
    "                \"Ts\": Ts\n",
    "            })\n",
    "\n",
    "            for frame_idx in frame_names:\n",
    "                self.index_map.append((seq_idx, frame_idx))\n",
    "\n",
    "        self.num_views = len(self.sequence_data[0][\"cam_dirs\"])\n",
    "\n",
    "        self.meta_files = sorted([\n",
    "            os.path.join(cropped_dir, seq_dir, f)\n",
    "            for seq_dir in sequences\n",
    "            for f in os.listdir(os.path.join(cropped_dir, seq_dir))\n",
    "            if f.endswith('_meta.pt')\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, frame_name = self.index_map[idx]\n",
    "        seq_info = self.sequence_data[seq_idx]\n",
    "\n",
    "        Ks, Ts = seq_info[\"Ks\"], seq_info[\"Ts\"]\n",
    "        person_ids = seq_info[\"frame_names\"][frame_name]\n",
    "        frame = frame_name\n",
    "        seq_name = seq_info[\"name\"]\n",
    "\n",
    "        N_max = len(person_ids)\n",
    "        cam_dirs = seq_info[\"cam_dirs\"]\n",
    "        V = len(cam_dirs)\n",
    "        crop_images = torch.full(\n",
    "            (V, N_max, 3, 256, 192),\n",
    "            float(\"nan\"),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        images = torch.full(\n",
    "            (V, 3, 1080, 1920),\n",
    "            float(\"nan\"),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        metas_all = [[None for _ in range(N_max)] for _ in range(V)]\n",
    "\n",
    "        for v, cam_dir in enumerate(seq_info[\"cam_dirs\"]):\n",
    "            valid_i = 0\n",
    "\n",
    "            img_path = os.path.join(cam_dir.replace('cropped', 'raw'), frame+'.png')\n",
    "            img = cv2.imread(img_path)\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "            images[v] = img\n",
    "            \n",
    "            for person_id in person_ids:\n",
    "                pid = int(person_id.split('person')[-1].split('.')[0])\n",
    "                crop_path = os.path.join(cam_dir, frame+'_'+person_id)\n",
    "\n",
    "                if not os.path.isfile(crop_path):\n",
    "                    continue\n",
    "\n",
    "                crop = cv2.imread(crop_path)\n",
    "                crop = torch.from_numpy(crop).permute(2, 0, 1).float() / 255.0\n",
    "                crop_images[v, valid_i] = crop\n",
    "\n",
    "                frame_idx = int(str(frame).split('_')[1])\n",
    "\n",
    "                meta_path = self.meta_files[seq_idx]\n",
    "                meta_data = torch.load(meta_path)\n",
    "                meta_frame = next(\n",
    "                    (m for m in meta_data if m['camera'] == os.path.basename(cam_dir) and m['frame'] == f'{frame_name}.png' and m['person'] == pid),\n",
    "                    None\n",
    "                )\n",
    "                metas_all[v][valid_i] = meta_frame\n",
    "                valid_i += 1\n",
    "\n",
    "        return {\n",
    "            \"images\": images,  # (V,C,H,W)\n",
    "            \"crops\": crop_images,  # (V,C,H,W)\n",
    "            \"metas\": metas_all,\n",
    "            \"Ks\": Ks,\n",
    "            \"Ts\": Ts,\n",
    "            \"seq_name\": seq_name,\n",
    "            \"frame_idx\": frame_idx,\n",
    "        }\n",
    "\n",
    "cropped_dir = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/dataset_finetune2d/cropped'\n",
    "raw_dir = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/dataset_finetune2d/raw'\n",
    "dataset = CroppedMultiViewDataset(cropped_dir, raw_dir, K=K, downsample=5)\n",
    "\n",
    "print('len =', len(dataset))  # total number of frames across all sequences\n",
    "sample = dataset[1]\n",
    "print(sample[\"crops\"].shape)  # (V,C,H,W)\n",
    "print(sample['metas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f849b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for batch in train_loader:\\n    for el in range(BATCH_SIZE):\\n        print(\\'\\n--- Element\\', el, \\'---\\')\\n        print(batch[\"crops\"].shape)  # (B, V, C, H, W)\\n        print(batch[\"seq_name\"][el])\\n        print(batch[\"frame_idx\"][el])\\n        #im = batch[\"crops\"][el]\\n        #print(im.shape)\\n        #plt.imshow(im[0][0].permute(1, 2, 0).numpy())\\n        #plt.show()\\n\\n    break '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_padded(batch):\n",
    "    \"\"\"\n",
    "    Collate function for multiview crops with variable N (persons per frame).\n",
    "    Pads missing detections with NaN.\n",
    "    Vectorized for images and crops.\n",
    "    \"\"\"\n",
    "    # === Extract items ===\n",
    "    images_list = [item[\"images\"] for item in batch]  # (V, C, H, W)\n",
    "    crops_list  = [item[\"crops\"]  for item in batch]  # (V, N_i, C, H, W)\n",
    "    Ks_list     = [item[\"Ks\"]     for item in batch]\n",
    "    Ts_list     = [item[\"Ts\"]     for item in batch]\n",
    "    metas_list  = [item[\"metas\"]  for item in batch]\n",
    "    seq_names   = [item[\"seq_name\"] for item in batch]\n",
    "    frame_idxs  = [item[\"frame_idx\"] for item in batch]\n",
    "\n",
    "    B = len(batch)\n",
    "    V = crops_list[0].shape[0]\n",
    "\n",
    "    # === Pad original images (vectorized) ===\n",
    "    padded_orig_images = torch.stack(images_list, dim=0)  # (B, V, C, H, W)\n",
    "\n",
    "    # === Pad crops (efficient, minimal loops) ===\n",
    "    N_max = max(c.shape[1] for c in crops_list)\n",
    "    C, H, W = crops_list[0].shape[2:]\n",
    "    padded_crops = torch.full((B, V, N_max, C, H, W), float(\"nan\"), dtype=torch.float32)\n",
    "    for i, c in enumerate(crops_list):\n",
    "        padded_crops[i, :, :c.shape[1]] = c\n",
    "\n",
    "    # === Pad metas ===\n",
    "    padded_metas = [\n",
    "        [[*(metas_b[v]), *([None] * (N_max - len(metas_b[v])))] for v in range(V)]\n",
    "        for metas_b in metas_list\n",
    "    ]\n",
    "\n",
    "    # === Stack Ks and Ts ===\n",
    "    Ks = torch.stack(Ks_list)\n",
    "    Ts = torch.stack(Ts_list)\n",
    "\n",
    "    return {\n",
    "        \"images\": padded_orig_images,  # (B, V, C, H, W)\n",
    "        \"crops\": padded_crops,         # (B, V, N, C, H, W)\n",
    "        \"metas\": padded_metas,\n",
    "        \"Ks\": Ks,\n",
    "        \"Ts\": Ts,\n",
    "        \"seq_name\": seq_names,\n",
    "        \"frame_idx\": frame_idxs\n",
    "    }\n",
    "\n",
    "# Dataloader\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size   = int(0.1 * len(dataset))\n",
    "test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # reproducible split\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True, collate_fn=collate_padded)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True, collate_fn=collate_padded)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_padded)\n",
    "\n",
    "\"\"\" for batch in train_loader:\n",
    "    for el in range(BATCH_SIZE):\n",
    "        print('\\n--- Element', el, '---')\n",
    "        print(batch[\"crops\"].shape)  # (B, V, C, H, W)\n",
    "        print(batch[\"seq_name\"][el])\n",
    "        print(batch[\"frame_idx\"][el])\n",
    "        #im = batch[\"crops\"][el]\n",
    "        #print(im.shape)\n",
    "        #plt.imshow(im[0][0].permute(1, 2, 0).numpy())\n",
    "        #plt.show()\n",
    "\n",
    "    break \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3ed98",
   "metadata": {},
   "source": [
    "### Bone length loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c1a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  shoulder-elbow  elbow-wrist  hip-knee  knee-ankle\n",
      "0   1             290          230       440         330\n",
      "1   2             285          260       520         360\n",
      "2   3             310          230       520         370\n",
      "3   4             285          250       520         350\n",
      "4   5             340          270       540         420\n",
      "5   6             390          270       530         370\n",
      "6   7             300          240       450         350\n"
     ]
    }
   ],
   "source": [
    "seg_lengths = [(1, 290, 230, 440, 330),\n",
    "               (2, 285, 260, 520, 360),\n",
    "               (3, 310, 230, 520, 370),\n",
    "               (4, 285, 250, 520, 350),\n",
    "               (5, 340, 270, 540, 420),\n",
    "               (6, 390, 270, 530, 370),\n",
    "               (7, 300, 240, 450, 350)]\n",
    "\n",
    "map_seg_to_keypoints = [(2, 3), (3, 4), (9, 10), (10, 11), (5, 6), (6, 7), (12, 13), (13, 14)]\n",
    "\n",
    "df = pd.DataFrame(seg_lengths)\n",
    "df.columns = [\"id\", \"shoulder-elbow\", \"elbow-wrist\", \"hip-knee\", \"knee-ankle\"]\n",
    "df.to_pickle(\"seg_lengths.pkl\")\n",
    "print(df)\n",
    "\n",
    "def get_loss_bones(seg_lengths, keypoints, map):\n",
    "    loss = 0\n",
    "    for side in range(2):\n",
    "        for i, l in enumerate(seg_lengths):\n",
    "            pair = map[side][i]\n",
    "            loss += abs(l - torch.linalg.norm(keypoints[pair[0]] - keypoints[pair[1]]))\n",
    "    return loss/8\n",
    "\n",
    "def bone_length_loss(keypoints, seg_lengths, map_seg_to_keypoints, device=None):\n",
    "    \"\"\"\n",
    "    keypoints: Tensor [B, N, D]  (D=2 or 3)\n",
    "    seg_lengths: list of per-bone target lengths (float)\n",
    "    map_seg_to_keypoints: list of tuples [(i,j), (k,l), ...]\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    map_seg = torch.tensor(map_seg_to_keypoints, device=device)\n",
    "    target_lengths = seg_lengths #, device=keypoints.device, dtype=torch.float32)\n",
    "\n",
    "    # Extract coordinates for bone endpoints\n",
    "    kp1 = keypoints[:, map_seg[:, 0], :]  # [B, n_bones, D]\n",
    "    kp2 = keypoints[:, map_seg[:, 1], :]  # [B, n_bones, D]\n",
    "\n",
    "    # Compute bone lengths per sample\n",
    "    bone_lengths = torch.linalg.norm(kp1 - kp2, dim=-1)  # [B, n_bones]\n",
    "\n",
    "    # Compute loss (L2 difference)\n",
    "    loss = torch.nanmean(torch.abs(bone_lengths - target_lengths))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87bda3",
   "metadata": {},
   "source": [
    "### GT from Pose2Sim 8 cam triangulation ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48058036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_analysis import extract_coordinates\n",
    "\n",
    "vit = ['Nose', 'REye', 'LEye', 'REar', 'LEar', 'RShoulder', 'LShoulder', 'RElbow', 'LElbow', 'RWrist', 'LWrist',\n",
    "       'RHip', 'LHip', 'RKnee', 'LKnee', 'RAnkle', 'LAnkle']\n",
    "pose2sim_vit = ['RHip', 'RKnee', 'RAnkle', 'LHip', 'LKnee', 'LAnkle', 'Nose',\n",
    "                'RShoulder', 'RElbow', 'RWrist', 'LShoulder', 'LElbow', 'LWrist']\n",
    "common_indices_vit = [j for j in vit if j in pose2sim_vit]\n",
    "matching_pose2sim_vit = [pose2sim_vit.index(j) for j in common_indices_vit]\n",
    "matching_vit = [vit.index(j) for j in common_indices_vit]\n",
    "\n",
    "def get_gt_coords(seqs, frames, rotate=True):\n",
    "   # define Rotation for Pose2Sim to World\n",
    "   R = np.array([\n",
    "      [0, 0, 1],\n",
    "      [-1, 0, 0],\n",
    "      [0, 1, 0]\n",
    "   ])\n",
    "\n",
    "   gt_coords_all = []\n",
    "\n",
    "   for seq, frame in zip(seqs, frames):\n",
    "      path = os.path.join(\n",
    "         '/home/lea/trampo/MODELS_2D3D/Pose2Sim/pose-3d-vit-multi',seq, 'GT_8cam', f'{seq}.trc')\n",
    "      gt_coords, gt_frames = extract_coordinates(path)\n",
    "\n",
    "      frame_val = frame.item() if torch.is_tensor(frame) else frame\n",
    "      mask = gt_frames == frame_val\n",
    "\n",
    "      if not np.any(mask):\n",
    "         # frame not found → create a NaN array of same shape as expected\n",
    "         n_joints = 13\n",
    "         nan_coords = np.full((n_joints, 3), np.nan, dtype=np.float32)\n",
    "         gt_coords_all.append(nan_coords)\n",
    "         continue\n",
    "\n",
    "      # frame found\n",
    "      gt_coords = gt_coords[mask, matching_pose2sim_vit, :]\n",
    "\n",
    "      if rotate:\n",
    "         gt_coords = (R @ gt_coords.T).T\n",
    "\n",
    "      gt_coords_all.append(gt_coords)\n",
    "\n",
    "   return gt_coords_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba3629",
   "metadata": {},
   "source": [
    "### Train + validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164152d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard log dir: runs/GTViT_03\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m predict_multiview_with_grad(pose_model, crops, metas, device\u001b[38;5;241m=\u001b[39mdevice, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# --- 3. Triangulate ---\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m error, preds_2d, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_triangulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(preds_2d)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m crops, Ks, Ts, keypoints, points_3d, preds_2d\n",
      "File \u001b[0;32m~/trampo/MODELS_2D3D/finetuning_multiview/utils.py:524\u001b[0m, in \u001b[0;36mfind_best_triangulation\u001b[0;34m(keypoints_per_view, Ks, Ts, error_threshold_tracking)\u001b[0m\n\u001b[1;32m    521\u001b[0m combinations_with_cams_off \u001b[38;5;241m=\u001b[39m combinations_with_cams_off\u001b[38;5;241m.\u001b[39mmasked_fill(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# --- Triangulate all subsets at once ---\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m error_comb_all, _, Q_comb_all \u001b[38;5;241m=\u001b[39m \u001b[43mtriangulate_comb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombinations_with_cams_off\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# --- Evaluate results ---\u001b[39;00m\n\u001b[1;32m    527\u001b[0m error_min \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(error_comb_all)  \u001b[38;5;66;03m# error_comb_all is already a tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/trampo/MODELS_2D3D/finetuning_multiview/utils.py:451\u001b[0m, in \u001b[0;36mtriangulate_comb\u001b[0;34m(combs, coords, Ks, Ts)\u001b[0m\n\u001b[1;32m    448\u001b[0m P_masked \u001b[38;5;241m=\u001b[39m P_batch \u001b[38;5;241m*\u001b[39m mask_expanded\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# (batch_size, N_cams, 3, 4)\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Triangulate valid combinations\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m Q_valid \u001b[38;5;241m=\u001b[39m \u001b[43mtriangulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_masked\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_combinations\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_masked\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_combinations\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Compute reprojection\u001b[39;00m\n\u001b[1;32m    454\u001b[0m q_calc, _ \u001b[38;5;241m=\u001b[39m project_points(Q_valid, P_masked[valid_combinations])\n",
      "File \u001b[0;32m~/trampo/MODELS_2D3D/finetuning_multiview/utils.py:357\u001b[0m, in \u001b[0;36mtriangulate\u001b[0;34m(points, P)\u001b[0m\n\u001b[1;32m    353\u001b[0m A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(N_combs \u001b[38;5;241m*\u001b[39m K, N_cams \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Batched SVD: returns U, S, Vh with shapes (n_kpts, m, m)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# We only need Vh\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m _, _, Vh \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# Last column of V (last row of Vh) is solution\u001b[39;00m\n\u001b[1;32m    360\u001b[0m Q_hom \u001b[38;5;241m=\u001b[39m Vh[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# (n_kpts, 4)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name = 'GTViT_03'\n",
    "# Create a log directory (TensorBoard will read from this)\n",
    "writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "print(\"TensorBoard log dir:\", writer.log_dir)\n",
    "\n",
    "error_thresh = 500\n",
    "person_dist_thresh = 500\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "os.makedirs(f\"checkpoints/{run_name}\", exist_ok=True)\n",
    "os.makedirs(f\"viz/{run_name}\", exist_ok=True)\n",
    "\n",
    "num_epochs = 5\n",
    "global_step = 0\n",
    "lambda_bones = 0.1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "seg_lengths = pd.read_pickle(\"seg_lengths.pkl\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    pose_model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        start_total = time.perf_counter()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # --- 0. Data transfer ---\n",
    "        t0 = time.perf_counter()\n",
    "        images, crops, metas, Ks, Ts, seq, frames = batch.values()\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        Ks = Ks.to(device, non_blocking=True)\n",
    "        Ts = Ts.to(device, non_blocking=True)\n",
    "        id = 1\n",
    "        seg_l_id = seg_lengths.loc[seg_lengths['id'] == id].iloc[0, 1:].to_numpy()\n",
    "        seg_l_tensor = torch.tensor(seg_l_id, dtype=torch.float32, device=device)\n",
    "        seg_l_tensor = torch.cat([seg_l_tensor, seg_l_tensor])\n",
    "\n",
    "        # --- 1. Get GT from Pose2Sim ---\n",
    "        gt_coords = get_gt_coords(seq, frames, rotate=True)\n",
    "        points_3d = torch.tensor(np.array(gt_coords), dtype=torch.float32, device=device)\n",
    "        if torch.isnan(points_3d).all():\n",
    "            del gt_coords, points_3d\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # --- 2. Predict 2D keypoints ---\n",
    "        keypoints = predict_multiview_with_grad(pose_model, crops, metas, device=device, training=True)\n",
    "        \n",
    "        # --- 3. Triangulate ---\n",
    "        error, preds_2d, _, _ = find_best_triangulation(keypoints, Ks, Ts, error_thresh)\n",
    "        if torch.isnan(preds_2d).all():\n",
    "            del crops, Ks, Ts, keypoints, points_3d, preds_2d\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        \n",
    "        # --- 4. Reprojection ---\n",
    "        Rt = Ts[:, :, :3, :]\n",
    "        P_all = Ks @ Rt\n",
    "        reproj, valid_mask = project_points(points_3d, P_all)\n",
    "\n",
    "        # --- 5. Filtering ---\n",
    "        preds_valid = preds_2d[..., matching_vit, :][valid_mask]\n",
    "        reproj_valid = reproj[valid_mask]\n",
    "        dist = torch.norm(preds_valid - reproj_valid, dim=-1)\n",
    "\n",
    "        keep_mask = dist < person_dist_thresh\n",
    "        if keep_mask.sum() == 0:\n",
    "            del images, Ks, Ts, detections, keypoints, preds_2d, points_3d, reproj\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        preds_valid = preds_valid[keep_mask]\n",
    "        reproj_valid = reproj_valid[keep_mask]\n",
    "        #print('pred:', preds_valid[0:5])\n",
    "        #print('reproj:', reproj_valid[0:5])\n",
    "\n",
    "        # --- 5. Losses ---\n",
    "        loss = torch.sqrt(torch.nn.functional.mse_loss(preds_valid, reproj_valid))\n",
    "        #loss_bones = bone_length_loss(points_3d, seg_l_tensor, map_seg_to_keypoints)\n",
    "        #loss = loss_reproj + lambda_bones * loss_bones\n",
    "\n",
    "        # --- 6. Backprop ---\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_step % 1 == 0:\n",
    "            #writer.add_scalar(\"Train/Loss_reproj\", loss_reproj.item(), global_step)\n",
    "            #writer.add_scalar(\"Train/Loss_bones\", loss_bones.item(), global_step)\n",
    "            writer.add_scalar(\"Train/Loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Train/Triangulation_error\", torch.nanmean(error).item(), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            images_np = images.cpu().detach().numpy()[0]\n",
    "            show_keypoints_on_im(images_np, preds_2d, reproj, f'viz/{run_name}/epoch{epoch+1}_it{step}', show=False)\n",
    "\n",
    "        # Memory cleanup after step\n",
    "        del Ks, Ts, keypoints, preds_2d, reproj, points_3d, preds_valid, reproj_valid, dist, keep_mask, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "        # --- Timing summary ---\n",
    "        \"\"\" total_time = t8 - start_total\n",
    "        print(\n",
    "            f\"[Step {step}] \"\n",
    "            f\"Data: {t1 - t0:.3f}s | \"\n",
    "            f\"Predict: {t3 - t2:.3f}s | \"\n",
    "            f\"Triangulate: {t4 - t3:.3f}s | \"\n",
    "            f\"Reproject: {t5 - t4:.3f}s | \"\n",
    "            f\"Filter: {t6 - t5:.3f}s | \"\n",
    "            f\"Loss: {t7 - t6:.3f}s | \"\n",
    "            f\"Backprop: {t8 - t7:.3f}s | \"\n",
    "            f\"Total: {total_time:.3f}s\"\n",
    "        ) \"\"\"\n",
    "\n",
    "     # --- VALIDATION ---\n",
    "    pose_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            images, crops, Ks, Ts, seq, frames, detections = val_batch.values()\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            crops = crops.to(device, non_blocking=True)\n",
    "            Ks = Ks.to(device, non_blocking=True)\n",
    "            Ts = Ts.to(device, non_blocking=True)\n",
    "            detections = detections.to(device, non_blocking=True)\n",
    "            \n",
    "            # --- 1. Predict 2D keypoints for detected views ---\n",
    "            keypoints = predict_multiview_with_grad(pose_model, crops, metas, device=device, training=True)\n",
    "            \n",
    "            # --- 2. Triangulate (batched) ---\n",
    "            error, preds_2d, points_3d, cams_on = find_best_triangulation(keypoints, Ks, Ts, error_thresh)\n",
    "            if torch.isnan(points_3d).all():\n",
    "                del images, Ks, Ts, detections, keypoints, points_3d\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            # --- 3. Reproject 3D back into each view ---\n",
    "            Rt = Ts[:, :, :3, :]\n",
    "            P_all = Ks @ Rt\n",
    "            reproj, valid_mask = project_points(points_3d, P_all)\n",
    "            preds_valid = preds_2d[valid_mask]\n",
    "            reproj_valid = reproj[valid_mask]\n",
    "\n",
    "            # --- 4. Remove mismatched persons (dist > 100) ---\n",
    "            dist = torch.norm(preds_valid - reproj_valid, dim=-1)  # Euclidean distance\n",
    "            keep_mask = dist < person_dist_thresh\n",
    "            if keep_mask.sum() == 0:\n",
    "                del images, Ks, Ts, detections, keypoints, preds_2d, reproj, points_3d\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            preds_valid = preds_valid[keep_mask]\n",
    "            reproj_valid = reproj_valid[keep_mask]\n",
    "\n",
    "            val_loss_batch = torch.sqrt(torch.nn.functional.mse_loss(preds_valid, reproj_valid))\n",
    "            val_loss += val_loss_batch.item()\n",
    "\n",
    "            # Memory cleanup after batch\n",
    "            del images, Ks, Ts, detections, keypoints, preds_2d, reproj, points_3d, preds_valid, reproj_valid, dist, keep_mask, val_loss_batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    writer.add_scalar(\"Val/Loss\", val_loss, epoch)\n",
    "\n",
    "    # --- Save checkpoint (optional: only best) ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': pose_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, f\"checkpoints/{run_name}/best_model.pth\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    ckpt_path = f\"checkpoints/{run_name}/epoch_{epoch+1}.pth\"\n",
    "    torch.save({ \n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': pose_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': val_loss,\n",
    "    }, ckpt_path)\n",
    "    print(f\"✅ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # Memory cleanup after epoch\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# TODO IDEA : per-camera loss for view-specific pose estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dbb2f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final test RMSE: 47.7418\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"checkpoints/GTViT_02/best_model.pth\", map_location=device)\n",
    "pose_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pose_model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, Ks, Ts, seq, frames, detections = batch.values()\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        Ks = Ks.to(device, non_blocking=True)\n",
    "        Ts = Ts.to(device, non_blocking=True)\n",
    "        detections = detections.to(device, non_blocking=True)\n",
    "\n",
    "        # --- 1. Predict 2D keypoints for detected views ---\n",
    "        keypoints = predict_multiview_with_grad(\n",
    "            det_model, pose_model, images, device=device,\n",
    "            bbox_thr=0.3, training=True, detections=detections)\n",
    "        \n",
    "        # --- 2. Triangulate (batched) ---\n",
    "        error, preds_2d, points_3d, cams_on = find_best_triangulation(keypoints, Ks, Ts, error_thresh)\n",
    "        if torch.isnan(points_3d).all():\n",
    "            continue\n",
    "\n",
    "        # --- 3. Reproject 3D back into each view ---\n",
    "        Rt = Ts[:, :, :3, :]\n",
    "        P_all = Ks @ Rt\n",
    "        reproj, valid_mask = project_points(points_3d, P_all)\n",
    "        preds_valid = preds_2d[valid_mask]\n",
    "        reproj_valid = reproj[valid_mask]\n",
    "\n",
    "        # --- 4. Remove mismatched persons (dist > 100) ---\n",
    "        dist = torch.norm(preds_valid - reproj_valid, dim=-1)  # Euclidean distance\n",
    "        keep_mask = dist < person_dist_thresh\n",
    "        if keep_mask.sum() == 0:\n",
    "            continue\n",
    "        preds_valid = preds_valid[keep_mask]\n",
    "        reproj_valid = reproj_valid[keep_mask]\n",
    "\n",
    "        loss_batch = torch.sqrt(torch.nn.functional.mse_loss(preds_valid, reproj_valid))\n",
    "        test_loss += loss_batch.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"✅ Final test RMSE: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fc850",
   "metadata": {},
   "source": [
    "### Show keypoints on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 1080, 1920) (8, 17, 2) (8, 17, 2)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m pts_r \u001b[38;5;241m=\u001b[39m reproj\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[b]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(images_np\u001b[38;5;241m.\u001b[39mshape, pts_d\u001b[38;5;241m.\u001b[39mshape, pts_r\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mshow_keypoints_on_im\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mviz/epoch1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/trampo/MODELS_2D3D/finetuning_multiview/utils.py:686\u001b[0m, in \u001b[0;36mshow_keypoints_on_im\u001b[0;34m(images, detections, reprojection, savepath, show)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow_keypoints_on_im\u001b[39m(images, detections, reprojection, savepath, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 686\u001b[0m     det \u001b[38;5;241m=\u001b[39m \u001b[43mdetections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    687\u001b[0m     reproj \u001b[38;5;241m=\u001b[39m reprojection\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (im, pt_d, pt_r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, det, reproj)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "\n",
    "images_np = images.cpu().detach().numpy()[b]\n",
    "pts_d = preds_2d.cpu().detach().numpy()[b]\n",
    "pts_r = reproj.cpu().detach().numpy()[b]\n",
    "\n",
    "print(images_np.shape, pts_d.shape, pts_r.shape)\n",
    "\n",
    "show_keypoints_on_im(images_np, pts_d, pts_r, 'viz/epoch1', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192dd5cc",
   "metadata": {},
   "source": [
    "## Test detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_bbox_on_image(image, bboxes):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)  # show image\n",
    "    for bbox in bboxes:\n",
    "        x, y = bbox[0], bbox[1]\n",
    "        width = bbox[2] - bbox[0]\n",
    "        height = bbox[3] - bbox[1]\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), width, height,\n",
    "            linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9bf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "path = '/home/lea/trampo/MODELS_2D3D/finetuning_multiview/dataset/1_partie_0429_003/Camera2_M11140'\n",
    "im_files = os.listdir(path)\n",
    "\n",
    "det = 0\n",
    "for i, im_file in tqdm(enumerate(im_files)):\n",
    "    im_name = os.path.join(path, im_file)\n",
    "    results = model(im_name, classes=[0], conf=0.3, verbose=False)\n",
    "    bboxes = results[0].boxes.xyxy\n",
    "    det += len(bboxes)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        plot_bbox_on_image(cv2.imread(im_name), bboxes.cpu().numpy())\n",
    "    \"\"\" \n",
    "    if len(bboxes) > 0:\n",
    "        print(bboxes)\n",
    "\n",
    "    for result in results:  # peut contenir plusieurs images\n",
    "        boxes = result.boxes  # toutes les bbox\n",
    "        for box in boxes:\n",
    "            cls = int(box.cls[0])          # classe prédite (0 = person dans COCO)\n",
    "            conf = float(box.conf[0])      # score de confiance\n",
    "            xyxy = box.xyxy[0].tolist()    # coordonnées [x1, y1, x2, y2] \"\"\"  \n",
    "\n",
    "print(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d08ca",
   "metadata": {},
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdet, mmpose, mmengine, mmcv\n",
    "\n",
    "print(\"Torch CUDA:\", torch.version.cuda)\n",
    "print(\"Torch device:\", torch.cuda.get_device_name(0))\n",
    "x = torch.ones(1, device='cuda')\n",
    "print(\"CUDA test OK:\", x.device)\n",
    "\n",
    "print(\"MMCV:\", mmcv.__version__)\n",
    "print(\"MMDetection:\", mmdet.__version__)\n",
    "print(\"MMPose:\", mmpose.__version__)\n",
    "print(\"MMEngine:\", mmengine.__version__)\n",
    "\n",
    "#print(type(img_np), img_np.shape if hasattr(img_np, \"shape\") else None)\n",
    "print(det_model.__class__)\n",
    "print(det_model.cfg.test_dataloader.dataset.pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
