{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lea/anaconda3/envs/vitpose/lib/python3.8/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/lea/anaconda3/envs/vitpose/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import warnings\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         process_mmdet_results, vis_pose_result)\n",
    "from mmpose.datasets import DatasetInfo\n",
    "\n",
    "try:\n",
    "    from mmdet.apis.inference import inference_detector, init_detector\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "    print('Import error')\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ViTPose/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TopDownMoE is not in the models registry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m det_model \u001b[38;5;241m=\u001b[39m init_detector(det_config, det_checkpoint, device\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Init pose model\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m pose_model \u001b[38;5;241m=\u001b[39m \u001b[43minit_pose_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# === Dataset meta (MMPose >= 1.0) ===\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pose_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pose_model\u001b[38;5;241m.\u001b[39mdataset_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose/lib/python3.8/site-packages/mmpose/apis/inference.py:43\u001b[0m, in \u001b[0;36minit_pose_model\u001b[0;34m(config, checkpoint, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig must be a filename or Config object, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_posenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# load model checkpoint\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     load_checkpoint(model, checkpoint, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose/lib/python3.8/site-packages/mmpose/models/builder.py:39\u001b[0m, in \u001b[0;36mbuild_posenet\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_posenet\u001b[39m(cfg):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build posenet.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPOSENETS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose/lib/python3.8/site-packages/mmcv/utils/registry.py:237\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose/lib/python3.8/site-packages/mmcv/cnn/builder.py:27\u001b[0m, in \u001b[0;36mbuild_model_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose/lib/python3.8/site-packages/mmcv/utils/registry.py:61\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m     obj_cls \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mget(obj_type)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj_type) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj_type):\n\u001b[1;32m     64\u001b[0m     obj_cls \u001b[38;5;241m=\u001b[39m obj_type\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TopDownMoE is not in the models registry'"
     ]
    }
   ],
   "source": [
    "det_config = \"ViTPose/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py\" \n",
    "det_checkpoint = \"ViTPose/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\"\n",
    "\n",
    "# ViTPose Large\n",
    "#pose_config = \"ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_large_coco_256x192.py\"  \n",
    "#pose_checkpoint = \"ViTPose/checkpoints/vitpose-l.pth\"\n",
    "\n",
    "# ViTPose++ Base\n",
    "pose_config = \"ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_base_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py\"  \n",
    "pose_checkpoint = \"ViTPose/checkpoints/vitpose++_base.pth\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "det_cat_id = 1 # Category id for bounding box detection model\n",
    "bbox_thr = 0.5 # Bounding box score threshold\n",
    "kpt_thr = 0.5 # Keypoint score threshold\n",
    "radius = 4 # Keypoint radius for visualization\n",
    "thickness = 1 # Link thickness for visualization\n",
    "\n",
    "\"\"\" det_model = init_detector(det_config, det_checkpoint, device=device.lower())\n",
    "\n",
    "# build the pose model from a config file and a checkpoint file\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint, device=device.lower())\n",
    "\n",
    "dataset = pose_model.cfg.data['test']['type']\n",
    "dataset_info = pose_model.cfg.data['test'].get('dataset_info', None)\n",
    "if dataset_info is None:\n",
    "    warnings.warn(\n",
    "        'Please set `dataset_info` in the config.'\n",
    "        'Check https://github.com/open-mmlab/mmpose/pull/663 for details.',\n",
    "        DeprecationWarning)\n",
    "else:\n",
    "    dataset_info = DatasetInfo(dataset_info)\n",
    " \"\"\"\n",
    "\n",
    "# Init detector\n",
    "det_model = init_detector(det_config, det_checkpoint, device=device.lower())\n",
    "\n",
    "# Init pose model\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint, device=device.lower())\n",
    "\n",
    "# === Dataset meta (MMPose >= 1.0) ===\n",
    "if hasattr(pose_model, \"dataset_meta\") and pose_model.dataset_meta is not None:\n",
    "    dataset_info = DatasetInfo(pose_model.dataset_meta)\n",
    "else:\n",
    "    dataset_info = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_openpose(json_file_path, keypoints, scores):\n",
    "    '''\n",
    "    Save the keypoints and scores to a JSON file in the OpenPose format\n",
    "\n",
    "    INPUTS:\n",
    "    - json_file_path: Path to save the JSON file\n",
    "    - keypoints: Detected keypoints\n",
    "    - scores: Confidence scores for each keypoint\n",
    "\n",
    "    OUTPUTS:\n",
    "    - JSON file with the detected keypoints and confidence scores in the OpenPose format\n",
    "    '''\n",
    "\n",
    "    # Prepare keypoints with confidence scores for JSON output\n",
    "    nb_detections = len(keypoints)\n",
    "    # print('results: ', keypoints, scores)\n",
    "    detections = []\n",
    "    for i in range(nb_detections): # nb of detected people\n",
    "        keypoints_with_confidence_i = []\n",
    "        for kp, score in zip(keypoints[i], scores[i]):\n",
    "            keypoints_with_confidence_i.extend([kp[0].item(), kp[1].item(), score.item()])\n",
    "        detections.append({\n",
    "                    \"person_id\": [-1],\n",
    "                    \"pose_keypoints_2d\": keypoints_with_confidence_i,\n",
    "                    \"face_keypoints_2d\": [],\n",
    "                    \"hand_left_keypoints_2d\": [],\n",
    "                    \"hand_right_keypoints_2d\": [],\n",
    "                    \"pose_keypoints_3d\": [],\n",
    "                    \"face_keypoints_3d\": [],\n",
    "                    \"hand_left_keypoints_3d\": [],\n",
    "                    \"hand_right_keypoints_3d\": []\n",
    "                })\n",
    "            \n",
    "    # Create JSON output structure\n",
    "    json_output = {\"version\": 1.3, \"people\": detections}\n",
    "    \n",
    "    # Save JSON output for each frame\n",
    "    json_output_dir = os.path.abspath(os.path.join(json_file_path, '..'))\n",
    "    if not os.path.isdir(json_output_dir): os.makedirs(json_output_dir)\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(json_output, json_file)\n",
    "\n",
    "\n",
    "def run_rotate(det_model, img_name, frame_names, out_img_root, keypoint_pkl_path, pose_dir):\n",
    "    drawn_frames = []\n",
    "    all_keypoints = {}  # frame_name → list of person dicts\n",
    "    frame_paths = [os.path.join(img_name, frame_name) for frame_name in frame_names]\n",
    "\n",
    "    for frame_idx, image_file in tqdm(enumerate(frame_paths)):\n",
    "        out_file = os.path.join(out_img_root, f'vis_{frame_names[frame_idx]}')\n",
    "\n",
    "        # rotate to portrait\n",
    "        \"\"\" img = cv2.imread(image_file)\n",
    "        rotated_img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "        tmp_fd, tmp_img_path = tempfile.mkstemp(suffix=\".png\")\n",
    "        os.close(tmp_fd)\n",
    "        cv2.imwrite(tmp_img_path, rotated_img) \"\"\"\n",
    "\n",
    "        mmdet_results = inference_detector(det_model, image_file)\n",
    "        person_results = process_mmdet_results(mmdet_results, det_cat_id)\n",
    "\n",
    "        for person in person_results:\n",
    "            person['bbox'][2] = max(person['bbox'][2], 1)  # éviter w=0\n",
    "            person['bbox'][3] = max(person['bbox'][3], 1)  # éviter h=0\n",
    "            person['dataset_idx'] = 0  # <-- hack pour ViTPose++\n",
    "\n",
    "        pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "            pose_model,\n",
    "            image_file,\n",
    "            person_results,\n",
    "            bbox_thr=bbox_thr,\n",
    "            format='xyxy',\n",
    "            dataset=dataset,\n",
    "            dataset_info=dataset_info,\n",
    "            return_heatmap=False,\n",
    "            outputs=None)\n",
    "\n",
    "        keypoints = [r['keypoints'][:, :2] for r in pose_results]\n",
    "        scores = [r['keypoints'][:, 2] for r in pose_results]\n",
    "\n",
    "        json_output_dir = os.path.join(pose_dir, f'{os.path.basename(img_name)}_json')\n",
    "        json_file_path = os.path.join(json_output_dir, f\"{os.path.splitext(os.path.basename(image_file))[0]}_{frame_idx:06d}.json\")\n",
    "        \n",
    "        save_to_openpose(json_file_path, keypoints, scores)\n",
    "\n",
    "        # Collect keypoints with person ID\n",
    "        keypoints_per_frame = []\n",
    "        for pid, result in enumerate(pose_results):\n",
    "            keypoints_per_frame.append({\n",
    "                \"id\": pid,\n",
    "                \"bbox\": result['bbox'],  # [x1, y1, x2, y2]\n",
    "                \"keypoints\": result['keypoints'].tolist()\n",
    "            })\n",
    "\n",
    "        all_keypoints[frame_names[frame_idx]] = keypoints_per_frame\n",
    "\n",
    "        vis_pose_result(\n",
    "            pose_model,\n",
    "            image_file,\n",
    "            pose_results,\n",
    "            dataset=dataset,\n",
    "            dataset_info=dataset_info,\n",
    "            kpt_score_thr=kpt_thr,\n",
    "            radius=radius,\n",
    "            thickness=thickness,\n",
    "            show=False,\n",
    "            out_file=out_file)\n",
    "\n",
    "        drawn_frame = cv2.imread(out_file)\n",
    "        drawn_frames.append(drawn_frame)\n",
    "\n",
    "        #os.remove(tmp_img_path)\n",
    "\n",
    "    # Save all keypoints as .pkl\n",
    "    with open(keypoint_pkl_path, 'wb') as f:\n",
    "        pickle.dump(all_keypoints, f)\n",
    "\n",
    "    return drawn_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert results to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_video(drawn_frames, out_img_root, img_name):\n",
    "    output_video_path = os.path.join(out_img_root, img_name.split('/')[-1].split('.')[0] + '_vit.mp4')\n",
    "    height, width, _ = drawn_frames[0].shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps=120.0, frameSize=(width, height))\n",
    "\n",
    "    for frame in drawn_frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "def convert_imfolder_to_video(images_path, video_path):\n",
    "    frame0 = cv2.imread(os.path.join(images_path, os.listdir(images_path)[0]))\n",
    "    height, width, _ = frame0.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps=120.0, frameSize=(width, height))\n",
    "\n",
    "    image_names = sorted(os.listdir(images_path), key=lambda x: int(str(x).split('.')[0][-5:]))\n",
    "\n",
    "    for file in image_names:\n",
    "        frame = cv2.imread(os.path.join(images_path, file))\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c3ac95ccaf4c32a644f24ce02da5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(path_im))):\n\u001b[1;32m      7\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_path_root, seq)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mconvert_imfolder_to_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_im\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mconvert_imfolder_to_video\u001b[0;34m(images_path, video_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m image_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(images_path), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m image_names:\n\u001b[0;32m---> 23\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (width, height))\n\u001b[1;32m     25\u001b[0m     video_writer\u001b[38;5;241m.\u001b[39mwrite(frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_im = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/20250429_vitL'\n",
    "video_path_root = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Results_video/20250429_vitL'\n",
    "if not os.path.isdir(video_path_root):\n",
    "    os.makedirs(video_path_root)\n",
    "\n",
    "for seq in tqdm(sorted(os.listdir(path_im))):\n",
    "    video_path = os.path.join(video_path_root, seq)\n",
    "    if not os.path.isfile(video_path+'.mp4'):\n",
    "        convert_imfolder_to_video(os.path.join(path_im, seq), video_path+'.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a8e0a8fdd7487981b6b3f135b24b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_partie_0429-Camera1_M11139\n",
      "1_partie_0429-Camera2_M11140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed33690bc6e40e394156929721f78d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'dataset_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m frame_names \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(img_name)\n\u001b[1;32m     19\u001b[0m frame_names\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 21\u001b[0m drawn_frames \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_img_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoint_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#convert_to_video(drawn_frames, out_img_root, img_name)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 68\u001b[0m, in \u001b[0;36mrun_rotate\u001b[0;34m(det_model, img_name, frame_names, out_img_root, keypoint_pkl_path, pose_dir)\u001b[0m\n\u001b[1;32m     65\u001b[0m     person[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(person[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# éviter h=0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     person[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# <-- hack pour ViTPose++\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m pose_results, returned_outputs \u001b[38;5;241m=\u001b[39m \u001b[43minference_top_down_pose_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperson_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_thr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxyxy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_heatmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m pose_results]\n\u001b[1;32m     80\u001b[0m scores \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m pose_results]\n",
      "File \u001b[0;32m~/trampo/vitpose/ViTPose/mmpose/apis/inference.py:400\u001b[0m, in \u001b[0;36minference_top_down_pose_model\u001b[0;34m(model, img_or_path, person_results, bbox_thr, format, dataset, dataset_info, return_heatmap, outputs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OutputHook(model, outputs\u001b[38;5;241m=\u001b[39moutputs, as_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m h:\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;66;03m# poses is results['pred'] # N x 17x 3\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m     poses, heatmap \u001b[38;5;241m=\u001b[39m \u001b[43m_inference_single_pose_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbboxes_xywh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_heatmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_heatmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_heatmap:\n\u001b[1;32m    409\u001b[0m         h\u001b[38;5;241m.\u001b[39mlayer_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheatmap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m heatmap\n",
      "File \u001b[0;32m~/trampo/vitpose/ViTPose/mmpose/apis/inference.py:278\u001b[0m, in \u001b[0;36m_inference_single_pose_model\u001b[0;34m(model, img_or_path, bboxes, dataset, dataset_info, return_heatmap)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_file\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m img_or_path\n\u001b[0;32m--> 278\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtest_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     batch_data\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    281\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m collate(batch_data, samples_per_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch_data))\n",
      "File \u001b[0;32m~/trampo/vitpose/ViTPose/mmpose/datasets/pipelines/shared_transform.py:99\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call function to apply transforms sequentially.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    dict: Transformed data.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 99\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/trampo/vitpose/ViTPose/mmpose/datasets/pipelines/shared_transform.py:168\u001b[0m, in \u001b[0;36mCollect.__call__\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m             key_src \u001b[38;5;241m=\u001b[39m key_tgt \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m--> 168\u001b[0m         meta[key_tgt] \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey_src\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_id\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    170\u001b[0m     meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset_idx'"
     ]
    }
   ],
   "source": [
    "path_root = \"/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/20250429\"\n",
    "out_root = path_root + \"_vit++B\"\n",
    "keypoint_path = path_root+ \"_keypts\"\n",
    "pose_dir = path_root + \"_pose\"\n",
    "\n",
    "os.makedirs(out_root, exist_ok=True)\n",
    "#os.makedirs(keypoint_path, exist_ok=True)\n",
    "os.makedirs(pose_dir, exist_ok=True)\n",
    "\n",
    "files = os.listdir(path_root)\n",
    "sorted_files = sorted(files)\n",
    "\n",
    "for i in tqdm(sorted_files):\n",
    "    print(i)\n",
    "    if i + '_vis' not in os.listdir(out_root):\n",
    "        img_name = os.path.join(path_root, i)\n",
    "        out_img_root = os.path.join(out_root, i + '_vis')\n",
    "        frame_names = os.listdir(img_name)\n",
    "        frame_names.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "        \n",
    "        drawn_frames = run_rotate(det_model, img_name, frame_names, out_img_root, keypoint_path+'/'+i+'.pkl', pose_dir)\n",
    "        #convert_to_video(drawn_frames, out_img_root, img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViTPose ++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n",
    "\n",
    "# Helper: save OpenPose-like JSON\n",
    "def save_to_openpose(json_path: str, keypoints_list: List[np.ndarray], scores_list: List[np.ndarray]):\n",
    "    \"\"\"\n",
    "    keypoints_list: list of (K,2)\n",
    "    scores_list: list of (K,)\n",
    "    Saves JSON in the simple OpenPose format:\n",
    "    { \"people\": [ {\"pose_keypoints_2d\": [x1,y1,s1, x2,y2,s2, ...]}, ... ] }\n",
    "    \"\"\"\n",
    "    people = []\n",
    "    for kps, sc in zip(keypoints_list, scores_list):\n",
    "        flat = []\n",
    "        for (x, y), s in zip(kps, sc):\n",
    "            flat.extend([float(x), float(y), float(s)])\n",
    "        people.append({\"pose_keypoints_2d\": flat})\n",
    "    out = {\"people\": people}\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "\n",
    "def draw_pose_on_image(img_bgr: np.ndarray, pose_results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Draw keypoints on BGR image.\n",
    "    pose_results: list of dicts with 'keypoints' (Kx3)\n",
    "    Returns image with skeletons drawn.\n",
    "    \"\"\"\n",
    "    img = img_bgr.copy()\n",
    "    for person in pose_results:\n",
    "        kpts = np.array(person[\"keypoints\"], dtype=float)  # (K,3)\n",
    "        # Draw keypoints\n",
    "        for x, y, s in kpts:\n",
    "            cv2.circle(img, (int(x), int(y)), 3, (0, 0, 255), -1)\n",
    "        # Optional: draw bbox around keypoints\n",
    "        x_min, y_min = kpts[:,0].min(), kpts[:,1].min()\n",
    "        x_max, y_max = kpts[:,0].max(), kpts[:,1].max()\n",
    "        cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n",
    "    return img\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DETECTOR_MODEL = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "POSE_MODEL = \"usyd-community/vitpose-plus-base\"\n",
    "DET_CONF_THR = 0.5\n",
    "POSE_KPT_THR = 0.5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ------------------------------------------\n",
    "\n",
    "# ------------------ MODEL ------------------\n",
    "print(\"Loading detector...\")\n",
    "person_image_processor = AutoProcessor.from_pretrained(DETECTOR_MODEL)\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(DETECTOR_MODEL).to(DEVICE)\n",
    "\n",
    "print(\"Loading pose model...\")\n",
    "image_processor = AutoProcessor.from_pretrained(POSE_MODEL)\n",
    "pose_model = VitPoseForPoseEstimation.from_pretrained(POSE_MODEL).to(DEVICE)\n",
    "# ------------------------------------------\n",
    "\n",
    "path = \"/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/20250429/\"\n",
    "out_path = \"/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/out_vitpose++_base\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "for seq in tqdm(os.listdir(path)):\n",
    "    if '0429' in str(seq) and str(seq) not in os.listdir(out_path):\n",
    "        IMAGES_DIR = path + str(seq)\n",
    "        OUT_DIR = out_path + f\"/{str(seq)}\"\n",
    "\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "        vis_dir = os.path.join(OUT_DIR, \"vis\")\n",
    "        json_dir = os.path.join(OUT_DIR, \"json\")\n",
    "        os.makedirs(vis_dir, exist_ok=True)\n",
    "        os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "        # Gather images\n",
    "        img_paths = sorted(glob.glob(os.path.join(IMAGES_DIR, \"*.*\")))\n",
    "        img_paths = [p for p in img_paths if p.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))]\n",
    "        if len(img_paths) == 0:\n",
    "            raise FileNotFoundError(\"Aucune image trouvée dans IMAGES_DIR\")\n",
    "\n",
    "        all_keypoints = {}  # mapping frame_name -> list of persons dicts\n",
    "        frame_names = [Path(p).stem for p in img_paths]\n",
    "\n",
    "        for frame_idx, img_path in enumerate(tqdm(img_paths, desc=\"Frames\")):\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_size = (image.height, image.width)\n",
    "\n",
    "            # --- Stage 1: person detection (RTDetr) ---\n",
    "            inputs = person_image_processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                det_outputs = person_model(**inputs)\n",
    "\n",
    "            # Post-process detection results (HF processor provides helper)\n",
    "            results = person_image_processor.post_process_object_detection(\n",
    "                det_outputs, target_sizes=torch.tensor([(image.height, image.width)], dtype=torch.long), threshold=DET_CONF_THR\n",
    "            )\n",
    "            det_result = results[0]\n",
    "            person_boxes = det_result[\"boxes\"][det_result[\"labels\"] == 0]  # select label 0 = person\n",
    "            person_boxes = person_boxes.cpu().numpy()\n",
    "\n",
    "            # Convert from (x1,y1,x2,y2) -> (x1,y1,w,h) for the vitpose processor\n",
    "            if person_boxes.size > 0:\n",
    "                person_boxes_xywh = person_boxes.copy()\n",
    "                person_boxes_xywh[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
    "                person_boxes_xywh[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
    "                # ensure w,h >= 1\n",
    "                person_boxes_xywh[:, 2] = np.maximum(person_boxes_xywh[:, 2], 1.0)\n",
    "                person_boxes_xywh[:, 3] = np.maximum(person_boxes_xywh[:, 3], 1.0)\n",
    "            else:\n",
    "                person_boxes_xywh = np.zeros((0, 4), dtype=np.float32)\n",
    "\n",
    "            # If no person detected, still save empty json and continue\n",
    "            if person_boxes_xywh.shape[0] == 0:\n",
    "                json_file = os.path.join(json_dir, f\"{Path(img_path).stem}.json\")\n",
    "                save_to_openpose(json_file, [], [])\n",
    "                all_keypoints[frame_names[frame_idx]] = []\n",
    "                # copy original image to vis folder (no poses)\n",
    "                out_vis = os.path.join(vis_dir, f\"{Path(img_path).stem}.png\")\n",
    "                img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(out_vis, img_cv)\n",
    "                continue\n",
    "\n",
    "            # --- Stage 2: ViTPose inference per image (top-down) ---\n",
    "            # The HF vitpose processor expects boxes as list-of-arrays per image\n",
    "            inputs_pose = image_processor(image, boxes=[person_boxes_xywh], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "            # MOE dataset index hack (like you did): model expects dataset_index (here set to 0)\n",
    "            # inputs_pose can be augmented in-place\n",
    "            inputs_pose[\"dataset_index\"] = torch.tensor([0], device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = pose_model(**inputs_pose)\n",
    "\n",
    "            # Post-process (HF helper) to get keypoints per person\n",
    "            # threshold filters out low-confidence kps in the post-processing stage\n",
    "            pose_results_list = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes_xywh], threshold=POSE_KPT_THR)\n",
    "\n",
    "            # pose_results_list is a list per image; take first (only one image)\n",
    "            pose_results = pose_results_list[0]  # list of dicts, one per person\n",
    "\n",
    "            # The HF postprocess returns keypoints in the coordinate system of original image.\n",
    "            # Ensure bbox field exists and is xyxy so downstream code matches your format\n",
    "            \"\"\" processed_pose_results = []\n",
    "            keypoints_for_openpose = []\n",
    "            scores_for_openpose = []\n",
    "\n",
    "            for pid, pr in enumerate(pose_results):\n",
    "                kpts = pr[\"keypoints\"]  # typically tensor/np array shape (K,3)\n",
    "                # The HF result may include 'bbox' in xywh or xyxy depending on version - check\n",
    "                bbox = pr.get(\"bbox\", None)\n",
    "                if bbox is None:\n",
    "                    # fallback: reconstruct bbox from the passed person_boxes_xywh\n",
    "                    if pid < len(person_boxes):\n",
    "                        x, y, w, h = person_boxes_xywh[pid]\n",
    "                        bbox_xyxy = [float(x), float(y), float(x + w), float(y + h)]\n",
    "                    else:\n",
    "                        bbox_xyxy = [0, 0, 0, 0]\n",
    "                else:\n",
    "                    # If bbox is xywh -> convert to xyxy\n",
    "                    bbox = np.array(bbox, dtype=float)\n",
    "                    if len(bbox) == 4:\n",
    "                        # decide if it's xywh or xyxy by heuristic: if x2 > width or y2 > height then it's probably xywh\n",
    "                        # We'll assume HF returns xywh (as we passed), so convert:\n",
    "                        x, y, w, h = bbox\n",
    "                        bbox_xyxy = [float(x), float(y), float(x + w), float(y + h)]\n",
    "                    else:\n",
    "                        bbox_xyxy = bbox.tolist()\n",
    "\n",
    "                # normalize types\n",
    "                kpts_arr = np.array(kpts, dtype=float)\n",
    "                if kpts_arr.ndim == 2 and kpts_arr.shape[1] == 2:\n",
    "                    # If scores missing, append zeros\n",
    "                    kpts_arr = np.concatenate([kpts_arr, np.zeros((kpts_arr.shape[0], 1), dtype=float)], axis=1)\n",
    "\n",
    "                processed_pose_results.append({\n",
    "                    \"id\": int(pid),\n",
    "                    \"bbox\": bbox_xyxy,\n",
    "                    \"keypoints\": kpts_arr.tolist(),  # list of [x,y,s]\n",
    "                })\n",
    "\n",
    "                keypoints_for_openpose.append(kpts_arr[:, :2])  # (K,2)\n",
    "                scores_for_openpose.append(kpts_arr[:, 2])      # (K,) \"\"\"\n",
    "            processed_pose_results = []\n",
    "            keypoints_for_openpose = []\n",
    "            scores_for_openpose = []\n",
    "\n",
    "            for pid, pr in enumerate(pose_results):\n",
    "                kpts_arr = np.array(pr[\"keypoints\"], dtype=float)  # (K,3) normalement\n",
    "\n",
    "                # Vérification et fallback si jamais il manque la 3ᵉ colonne\n",
    "                if kpts_arr.ndim == 2 and kpts_arr.shape[1] == 2:\n",
    "                    print(\"⚠️ Aucun score renvoyé par HF, ajout de -1 comme placeholder\")\n",
    "                    kpts_arr = np.concatenate(\n",
    "                        [kpts_arr, np.ones((kpts_arr.shape[0], 1), dtype=float) * -1],\n",
    "                        axis=1\n",
    "                    )\n",
    "\n",
    "                # Logging debug : afficher 3 premiers keypoints avec score\n",
    "                if pid == 0:\n",
    "                    print(f\"[DEBUG] Frame {frame_names[frame_idx]} - Person {pid} - sample kpts: {kpts_arr[:3]}\")\n",
    "\n",
    "                # bbox\n",
    "                bbox = pr.get(\"bbox\", None)\n",
    "                if bbox is None:\n",
    "                    if pid < len(person_boxes):\n",
    "                        x, y, w, h = person_boxes_xywh[pid]\n",
    "                        bbox_xyxy = [float(x), float(y), float(x + w), float(y + h)]\n",
    "                    else:\n",
    "                        bbox_xyxy = [0, 0, 0, 0]\n",
    "                else:\n",
    "                    bbox = np.array(bbox, dtype=float)\n",
    "                    if len(bbox) == 4:\n",
    "                        x, y, w, h = bbox\n",
    "                        bbox_xyxy = [float(x), float(y), float(x + w), float(y + h)]\n",
    "                    else:\n",
    "                        bbox_xyxy = bbox.tolist()\n",
    "\n",
    "                processed_pose_results.append({\n",
    "                    \"id\": int(pid),\n",
    "                    \"bbox\": bbox_xyxy,\n",
    "                    \"keypoints\": kpts_arr.tolist(),  # [x,y,score]\n",
    "                })\n",
    "\n",
    "                keypoints_for_openpose.append(kpts_arr[:, :2])  # (K,2)\n",
    "                scores_for_openpose.append(kpts_arr[:, 2])      # (K,)\n",
    "\n",
    "            # Save per-image JSON (OpenPose-like)\n",
    "            json_file = os.path.join(json_dir, f\"{Path(img_path).stem}.json\")\n",
    "            save_to_openpose(json_file, keypoints_for_openpose, scores_for_openpose)\n",
    "\n",
    "            # Collect for global pkl\n",
    "            all_keypoints[frame_names[frame_idx]] = processed_pose_results\n",
    "\n",
    "            # Visualization: draw and save\n",
    "            img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "            drawn = draw_pose_on_image(img_cv, processed_pose_results)\n",
    "            out_vis = os.path.join(vis_dir, f\"{Path(img_path).stem}.png\")\n",
    "            cv2.imwrite(out_vis, drawn)\n",
    "\n",
    "        # Save global pickle of all keypoints\n",
    "        pkl_path = os.path.join(OUT_DIR, \"all_keypoints.pkl\")\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(all_keypoints, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with MMPose (VitPose++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmpose.apis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmpose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inference_topdown, init_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmpose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_all_modules\n\u001b[1;32m      4\u001b[0m register_all_modules()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmpose.apis'"
     ]
    }
   ],
   "source": [
    "from mmpose.apis import inference_topdown, init_model\n",
    "from mmpose.utils import register_all_modules\n",
    "\n",
    "register_all_modules()\n",
    "\n",
    "config_file = 'td-hm_hrnet-w48_8xb32-210e_coco-256x192.py'\n",
    "checkpoint_file = 'td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'\n",
    "model = init_model(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "\n",
    "# please prepare an image with person\n",
    "results = inference_topdown(model, 'demo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ViTPose/checkpoints/vitpose++_base.pth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TopDownMoE is not in the mmpose::model registry. Please check whether the value of `TopDownMoE` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m pose_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/lea/trampo/vitpose/ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_small_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m pose_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViTPose/checkpoints/vitpose++_base.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m \u001b[43mMMPoseInferencer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ou 'cpu'\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Exemple d’inférence sur une image\u001b[39;00m\n\u001b[1;32m     13\u001b[0m results \u001b[38;5;241m=\u001b[39m inferencer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemo/demo.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, draw_heatmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/mmpose_inferencer.py:100\u001b[0m, in \u001b[0;36mMMPoseInferencer.__init__\u001b[0;34m(self, pose2d, pose2d_weights, pose3d, pose3d_weights, device, scope, det_model, det_weights, det_cat_ids, show_progress)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferencer \u001b[38;5;241m=\u001b[39m Pose3DInferencer(pose3d, pose3d_weights,\n\u001b[1;32m     95\u001b[0m                                            pose2d, pose2d_weights,\n\u001b[1;32m     96\u001b[0m                                            device, scope, det_model,\n\u001b[1;32m     97\u001b[0m                                            det_weights, det_cat_ids,\n\u001b[1;32m     98\u001b[0m                                            show_progress)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pose2d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferencer \u001b[38;5;241m=\u001b[39m \u001b[43mPose2DInferencer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose2d_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdet_cat_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither 2d or 3d pose estimation algorithm \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    105\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/pose2d_inferencer.py:86\u001b[0m, in \u001b[0;36mPose2DInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, det_model, det_weights, det_cat_ids, show_progress)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     76\u001b[0m              model: Union[ModelType, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     77\u001b[0m              weights: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m              det_cat_ids: Optional[Union[\u001b[38;5;28mint\u001b[39m, Tuple]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m              show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     init_default_scope(scope)\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m revert_sync_batchnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# assign dataset metainfo to self.visualizer\u001b[39;00m\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/base_mmpose_inferencer.py:64\u001b[0m, in \u001b[0;36mBaseMMPoseInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, show_progress)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     59\u001b[0m              model: Union[ModelType, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m              weights: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     61\u001b[0m              device: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m              scope: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m              show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/infer/infer.py:180\u001b[0m, in \u001b[0;36mBaseInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, show_progress)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pipeline(cfg)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_collate(cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/infer/infer.py:483\u001b[0m, in \u001b[0;36mBaseInferencer._init_model\u001b[0;34m(self, cfg, weights, device)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpretrained\n\u001b[0;32m--> 483\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m model\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_weights_to_model(model, checkpoint, cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/build_functions.py:232\u001b[0m, in \u001b[0;36mbuild_model_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/build_functions.py:100\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     obj_cls \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mget(obj_type)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry\u001b[38;5;241m.\u001b[39mscope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registry. \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease check whether the value of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect or it was registered as expected. More details \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan be found at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# this will include classes, functions, partial functions and more\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(obj_type):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TopDownMoE is not in the mmpose::model registry. Please check whether the value of `TopDownMoE` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'"
     ]
    }
   ],
   "source": [
    "from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "pose_config = \"/home/lea/trampo/vitpose/ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_small_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py\"\n",
    "pose_checkpoint = \"ViTPose/checkpoints/vitpose++_base.pth\"\n",
    "\n",
    "inferencer = MMPoseInferencer(\n",
    "    pose_config,\n",
    "    pose_checkpoint,\n",
    "    device='cuda:0'  # ou 'cpu'\n",
    ")\n",
    "\n",
    "# Exemple d’inférence sur une image\n",
    "results = inferencer('demo/demo.jpg', show=True, draw_heatmap=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "import mmpose\n",
    "print(mmpose.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ViTPose/checkpoints/vitpose++_base.pth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TopDownMoE is not in the mmpose::model registry. Please check whether the value of `TopDownMoE` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m POSE_CHECKPOINT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViTPose/checkpoints/vitpose++_base.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize inferencer for top-down pose estimation\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m \u001b[43mMMPoseInferencer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPOSE_CONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify the ViTPose++ model\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose2d_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPOSE_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdet_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDET_CONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify the object detector\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdet_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDET_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/mmpose_inferencer.py:100\u001b[0m, in \u001b[0;36mMMPoseInferencer.__init__\u001b[0;34m(self, pose2d, pose2d_weights, pose3d, pose3d_weights, device, scope, det_model, det_weights, det_cat_ids, show_progress)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferencer \u001b[38;5;241m=\u001b[39m Pose3DInferencer(pose3d, pose3d_weights,\n\u001b[1;32m     95\u001b[0m                                            pose2d, pose2d_weights,\n\u001b[1;32m     96\u001b[0m                                            device, scope, det_model,\n\u001b[1;32m     97\u001b[0m                                            det_weights, det_cat_ids,\n\u001b[1;32m     98\u001b[0m                                            show_progress)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pose2d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferencer \u001b[38;5;241m=\u001b[39m \u001b[43mPose2DInferencer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose2d_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdet_cat_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither 2d or 3d pose estimation algorithm \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    105\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/pose2d_inferencer.py:86\u001b[0m, in \u001b[0;36mPose2DInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, det_model, det_weights, det_cat_ids, show_progress)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     76\u001b[0m              model: Union[ModelType, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     77\u001b[0m              weights: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m              det_cat_ids: Optional[Union[\u001b[38;5;28mint\u001b[39m, Tuple]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m              show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     init_default_scope(scope)\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m revert_sync_batchnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# assign dataset metainfo to self.visualizer\u001b[39;00m\n",
      "File \u001b[0;32m~/trampo/mmpose/mmpose/apis/inferencers/base_mmpose_inferencer.py:64\u001b[0m, in \u001b[0;36mBaseMMPoseInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, show_progress)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     59\u001b[0m              model: Union[ModelType, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m              weights: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     61\u001b[0m              device: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m              scope: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m              show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/infer/infer.py:180\u001b[0m, in \u001b[0;36mBaseInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, show_progress)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pipeline(cfg)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_collate(cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/infer/infer.py:483\u001b[0m, in \u001b[0;36mBaseInferencer._init_model\u001b[0;34m(self, cfg, weights, device)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpretrained\n\u001b[0;32m--> 483\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m model\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_weights_to_model(model, checkpoint, cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/build_functions.py:232\u001b[0m, in \u001b[0;36mbuild_model_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vitpose_mmpose/lib/python3.10/site-packages/mmengine/registry/build_functions.py:100\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     obj_cls \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mget(obj_type)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry\u001b[38;5;241m.\u001b[39mscope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registry. \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease check whether the value of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect or it was registered as expected. More details \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan be found at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# this will include classes, functions, partial functions and more\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(obj_type):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TopDownMoE is not in the mmpose::model registry. Please check whether the value of `TopDownMoE` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "DET_CONFIG = \"ViTPose/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py\"\n",
    "DET_CHECKPOINT = \"ViTPose/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\"\n",
    "\n",
    "POSE_CONFIG = \"ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_base_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py\"\n",
    "POSE_CHECKPOINT = \"ViTPose/checkpoints/vitpose++_base.pth\"\n",
    "\n",
    "# Initialize inferencer for top-down pose estimation\n",
    "inferencer = MMPoseInferencer(\n",
    "    pose2d=POSE_CONFIG, # Specify the ViTPose++ model\n",
    "    pose2d_weights = POSE_CHECKPOINT,\n",
    "    det_model=DET_CONFIG, # Specify the object detector\n",
    "    det_weights=DET_CHECKPOINT,\n",
    "    device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmmpose\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     init_pose_model,\n\u001b[32m     11\u001b[39m     inference_top_down_pose_model,\n\u001b[32m     12\u001b[39m     vis_pose_result,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmmdet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector, process_mmdet_results\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mmpose'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mmpose.apis import (\n",
    "    init_pose_model,\n",
    "    inference_top_down_pose_model,\n",
    "    vis_pose_result,\n",
    ")\n",
    "from mmdet.apis import init_detector, inference_detector, process_mmdet_results\n",
    "import json\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DET_CONFIG = \"demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py\"\n",
    "DET_CHECKPOINT = \"checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\"\n",
    "\n",
    "POSE_CONFIG = \"configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_base_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py\"\n",
    "POSE_CHECKPOINT = \"checkpoints/vitpose++_base.pth\"\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "DET_CONF_THR = 0.5\n",
    "POSE_KPT_THR = 0.3\n",
    "# ---------------------------------------\n",
    "\n",
    "# Initialize models\n",
    "print(\"Loading detector...\")\n",
    "det_model = init_detector(DET_CONFIG, DET_CHECKPOINT, device=DEVICE)\n",
    "\n",
    "print(\"Loading pose model...\")\n",
    "pose_model = init_pose_model(POSE_CONFIG, POSE_CHECKPOINT, device=DEVICE)\n",
    "\n",
    "# Paths\n",
    "root_path = \"/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/\"\n",
    "out_root = os.path.join(root_path, \"out_vitpose++_mmpose\")\n",
    "os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "# Helper: save OpenPose-like JSON\n",
    "def save_to_openpose(json_path, keypoints_list):\n",
    "    people = []\n",
    "    for kps in keypoints_list:\n",
    "        flat = []\n",
    "        for x, y, s in kps['keypoints']:\n",
    "            flat.extend([float(x), float(y), float(s)])\n",
    "        people.append({\"pose_keypoints_2d\": flat})\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump({\"people\": people}, f, indent=2)\n",
    "\n",
    "# ----------------- LOOP OVER SEQUENCES -----------------\n",
    "sequences = sorted(set([d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))]))\n",
    "for seq in sequences:\n",
    "    seq_path = os.path.join(root_path, seq)\n",
    "    out_seq_dir = os.path.join(out_root, seq)\n",
    "    vis_dir = os.path.join(out_seq_dir, \"vis\")\n",
    "    json_dir = os.path.join(out_seq_dir, \"json\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "    # Gather images\n",
    "    img_paths = sorted(glob.glob(os.path.join(seq_path, \"*.*\")))\n",
    "    img_paths = [p for p in img_paths if p.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))]\n",
    "    if len(img_paths) == 0:\n",
    "        print(f\"No images found in {seq_path}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    all_keypoints = {}  # frame_name -> list of persons dict\n",
    "    frame_names = [Path(p).stem for p in img_paths]\n",
    "\n",
    "    # Loop over frames\n",
    "    for frame_idx, img_path in enumerate(tqdm(img_paths, desc=f\"Processing {seq}\")):\n",
    "        frame_name = frame_names[frame_idx]\n",
    "\n",
    "        # --- Stage 1: detect persons ---\n",
    "        mmdet_results = inference_detector(det_model, img_path)\n",
    "        person_results = process_mmdet_results(mmdet_results, cat_id=1)  # cat_id=1 -> person\n",
    "\n",
    "        for person in person_results:\n",
    "            person['bbox'][2] = max(person['bbox'][2], 1)\n",
    "            person['bbox'][3] = max(person['bbox'][3], 1)\n",
    "            person['dataset_idx'] = 0  # required for ViTPose++\n",
    "\n",
    "        # --- Stage 2: pose estimation ---\n",
    "        if len(person_results) > 0:\n",
    "            pose_results, _ = inference_top_down_pose_model(\n",
    "                pose_model,\n",
    "                img_path,\n",
    "                person_results,\n",
    "                bbox_thr=DET_CONF_THR,\n",
    "                format='xyxy',\n",
    "                dataset='TopDownCocoDataset',\n",
    "                return_heatmap=False\n",
    "            )\n",
    "        else:\n",
    "            pose_results = []\n",
    "\n",
    "        # Collect keypoints + scores\n",
    "        keypoints_per_frame = []\n",
    "        for pid, r in enumerate(pose_results):\n",
    "            keypoints_per_frame.append({\n",
    "                \"id\": pid,\n",
    "                \"bbox\": r['bbox'],            # [x1,y1,x2,y2]\n",
    "                \"keypoints\": r['keypoints'].tolist()  # (K,3) [x,y,score]\n",
    "            })\n",
    "        all_keypoints[frame_name] = keypoints_per_frame\n",
    "\n",
    "        # --- Save JSON ---\n",
    "        json_file = os.path.join(json_dir, f\"{frame_name}.json\")\n",
    "        save_to_openpose(json_file, keypoints_per_frame)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        img_vis = vis_pose_result(\n",
    "            pose_model,\n",
    "            img_path,\n",
    "            pose_results,\n",
    "            dataset='TopDownCocoDataset',\n",
    "            kpt_score_thr=POSE_KPT_THR,\n",
    "            show=False\n",
    "        )\n",
    "        out_vis_file = os.path.join(vis_dir, f\"{frame_name}.png\")\n",
    "        cv2.imwrite(out_vis_file, img_vis)\n",
    "\n",
    "    # Save global pickle for the sequence\n",
    "    pkl_path = os.path.join(out_seq_dir, \"all_keypoints.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(all_keypoints, f)\n",
    "\n",
    "    print(f\"Finished sequence {seq}, saved {len(img_paths)} frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dist_path = '/home/lea/trampo/Pose2Sim/pose_all_vit++'\n",
    "src_path = '/mnt/D494C4CF94C4B4F0/Trampoline_avril2025/Images_trampo_avril2025/20250429_vit++B'\n",
    "\n",
    "os.makedirs(dist_path, exist_ok=True)\n",
    "\n",
    "for seq in os.listdir(src_path):\n",
    "    if os.path.isdir(os.path.join(src_path, seq)):\n",
    "        os.mkdir(os.path.join(dist_path, seq))\n",
    "        for file in os.listdir(os.path.join(src_path, seq, 'json')):\n",
    "            shutil.copy2(os.path.join(src_path, seq, 'json', file), os.path.join(dist_path, seq, file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
